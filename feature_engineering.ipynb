{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c1",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Feature Engineering for Fraud Detection\n\nThis notebook demonstrates the value of feature engineering with Snowflake's Feature Store, Model Registry, and Experiment Tracking.\n\n**Demo Structure:**\n1. **Baseline Model** - XGBoost with basic feature preprocessing\n2. **Feature Store Model** - XGBoost with Customer & Terminal entities and feature views\n3. **Model Comparison** - Compare models and set best as default in registry"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "c2",
      "metadata": {
        "language": "python"
      },
      "source": "import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "exp_setup",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## Setup: Experiment Tracking & Model Registry\n\nInitialize experiment tracking at the start to capture all model training runs."
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "exp_init",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.ml.experiment import ExperimentTracking\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.feature_store import FeatureStore, Entity, FeatureView, CreationMode\nfrom snowflake.ml.model import task\nfrom snowflake.ml.model.model_signature import infer_signature\nfrom snowflake.ml.experiment.callback.xgboost import SnowflakeXgboostCallback\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nDATABASE = \"ML_DEMO\"\nSCHEMA = \"FRAUD_DETECTION\"\n\nsession.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE}\").collect()\nsession.sql(f\"CREATE SCHEMA IF NOT EXISTS {DATABASE}.{SCHEMA}\").collect()\nsession.sql(f\"USE DATABASE {DATABASE}\").collect()\nsession.sql(f\"USE SCHEMA {SCHEMA}\").collect()\n\nexp = ExperimentTracking(session=session)\nexp.delete_experiment(\"FRAUD_DETECTION_FEATURE_ENGINEERING\")\nexp.set_experiment(\"FRAUD_DETECTION_FEATURE_ENGINEERING\")\n\nreg = Registry(session=session, database_name=DATABASE, schema_name=SCHEMA)\n\nprint(\"✓ Experiment tracking initialized\")\nprint(\"✓ Model registry connected\")",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "c3",
      "metadata": {
        "language": "python"
      },
      "source": "df = pd.read_csv('data/transactions.csv')\nprint(f\"Dataset shape: {df.shape}\")\ndf.head()",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "c4",
      "metadata": {
        "language": "python"
      },
      "source": "# Create table from pandas dataframe and then load as Snowpark DataFrame\nsession.write_pandas(df, 'TRANSACTIONS_RAW', auto_create_table=True, overwrite=True)\ndf = session.table('TRANSACTIONS_RAW')\n\nprint(f\"Table TRANSACTIONS_RAW created with {df.count()} records\")\ndf.schema",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c5",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## Feature Engineering\n\nCreating time-based and rolling window features for fraud detection:"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "c6",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark import Window\nfrom snowflake.snowpark.functions import hour, dayofweek, when, col, count, sum, make_interval, to_timestamp\n\ndf = df.with_column('TX_DATETIME', to_timestamp('TX_DATETIME'))\n\ndf = df.with_columns(\n    ['HOUR', 'DAY_OF_WEEK', 'IS_WEEKEND', 'IS_NIGHT_12AM_7AM'],\n    [\n        hour('TX_DATETIME'),\n        dayofweek('TX_DATETIME'),\n        when(dayofweek('TX_DATETIME') >= 6, True).otherwise(False),\n        when(hour('TX_DATETIME').between(0, 6), True).otherwise(False)\n    ]\n)",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c15",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## View Feature Summary"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "c16",
      "metadata": {
        "language": "python"
      },
      "source": "row_count = df.count()\ncol_count = len(df.columns)\nprint(f\"\\nFinal dataset shape: ({row_count}, {col_count})\")\nprint(f\"\\nFeatures created:\")\nfeature_cols = ['IS_WEEKEND', 'IS_NIGHT_12AM_7AM', 'DAY_OF_WEEK', 'HOUR']\nfor c in feature_cols:\n    print(f\"  - {c}\")\n\nprint(\"\\nSample of features:\")\ndf.select(['TX_DATETIME', 'CUSTOMER_ID', 'TERMINAL_ID', 'TX_AMOUNT'] + feature_cols).limit(10).show()",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "c17",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\nFeature statistics:\")\ndf.select(feature_cols).describe().show()",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "step1_header",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n# Step 1: Baseline Model - Basic Feature Preprocessing\n\nBuild an XGBoost classifier with only basic features (no feature store)."
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "step1_prep",
      "metadata": {
        "language": "python"
      },
      "source": "df_pandas = df.to_pandas()\n\nbasic_features = ['TX_AMOUNT', 'HOUR', 'DAY_OF_WEEK', 'IS_WEEKEND', 'IS_NIGHT_12AM_7AM']\nX_basic = df_pandas[basic_features].astype(float)\ny = df_pandas['TX_FRAUD'].astype(int)\n\nX_train_basic, X_test_basic, y_train, y_test = train_test_split(\n    X_basic, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training set: {X_train_basic.shape[0]} samples\")\nprint(f\"Test set: {X_test_basic.shape[0]} samples\")\nprint(f\"Features: {basic_features}\")",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "step1_train",
      "metadata": {
        "language": "python"
      },
      "source": "model_baseline = XGBClassifier(\n    n_estimators=100,       # fixed, predictable\n    max_depth=5,\n    learning_rate=0.1,\n    random_state=42,\n    eval_metric=\"auc\",\n    tree_method=\"hist\",\n    n_jobs=-1\n)\n\nRUN_NAME = \"baseline_basic_features_demo_1\"\n\nwith exp.start_run(RUN_NAME):\n    # Log minimal, clear params\n    exp.log_params({\n        \"model_type\": \"XGBClassifier\",\n        \"feature_set\": \"basic\",\n        \"n_features\": len(basic_features),\n        \"n_train_rows\": len(X_train_basic),\n        \"n_test_rows\": len(X_test_basic),\n        \"n_estimators\": 100,\n        \"max_depth\": 5,\n        \"learning_rate\": 0.1,\n        \"eval_metric\": \"auc\"\n    })\n\n    model_baseline.fit(\n        X_train_basic,\n        y_train,\n        eval_set=[(X_test_basic, y_test)],\n        verbose=False\n    )\n\n    y_pred_baseline = model_baseline.predict(X_test_basic)\n    y_proba_baseline = model_baseline.predict_proba(X_test_basic)[:, 1]\n\n    metrics_baseline = {\n        \"accuracy\": float(accuracy_score(y_test, y_pred_baseline)),\n        \"precision\": float(precision_score(y_test, y_pred_baseline, zero_division=0)),\n        \"recall\": float(recall_score(y_test, y_pred_baseline, zero_division=0)),\n        \"f1_score\": float(f1_score(y_test, y_pred_baseline, zero_division=0)),\n        \"roc_auc\": float(roc_auc_score(y_test, y_proba_baseline))\n    }\n    exp.log_metrics(metrics_baseline)\n\n    mv_baseline = reg.log_model(\n        model_baseline,\n        model_name=\"FRAUD_DETECTION_MODEL\",\n        version_name=\"v1_demo_basic\",\n        sample_input_data=X_train_basic.iloc[: min(50, len(X_train_basic))],\n        task=task.Task.TABULAR_BINARY_CLASSIFICATION,\n        metrics=metrics_baseline,\n        comment=\"Demo baseline model with basic features\"\n    )\n\nprint(\"\\n=== Baseline Demo Model Results ===\")\nfor metric, value in metrics_baseline.items():\n    print(f\"{metric}: {value:.4f}\")",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "step2_header",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n# Step 2: Feature Store Model - Customer & Terminal Entities\n\nCreate entities and feature views for Customer and Terminal, then build model with enriched features."
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "step2_fs_setup",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.ml.feature_store import FeatureStore, Entity, FeatureView, CreationMode\n\nfs = FeatureStore(\n    session=session,\n    database=DATABASE,\n    name=SCHEMA,\n    default_warehouse=\"COMPUTE_WH\",\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)\n\nprint(\"✓ Feature Store initialized\")",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "step2_entities",
      "metadata": {
        "language": "python"
      },
      "source": "customer_entity = Entity(\n    name=\"CUSTOMER\",\n    join_keys=[\"CUSTOMER_ID\"],\n    desc=\"Customer entity for fraud detection features\"\n)\n\nterminal_entity = Entity(\n    name=\"TERMINAL\",\n    join_keys=[\"TERMINAL_ID\"],\n    desc=\"Terminal/ATM entity for fraud detection features\"\n)\n\ntry:\n    fs.register_entity(customer_entity)\n    print(\"✓ Customer entity registered\")\nexcept:\n    print(\"Customer entity already exists\")\n\ntry:\n    fs.register_entity(terminal_entity)\n    print(\"✓ Terminal entity registered\")\nexcept:\n    print(\"Terminal entity already exists\")\n\nfs.list_entities().show()",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "step2_save_source",
      "metadata": {
        "language": "python"
      },
      "source": "# Add customer rolling window features using Snowpark DataFrame API\n# This demonstrates the DataFrame approach (vs SQL shown in terminal features)\nprint(\"Adding customer rolling window features using Snowpark DataFrame API...\")\n\ndf = df.with_columns(\n    ['CUSTOMER_TX_COUNT_1H', 'CUSTOMER_TX_COUNT_1W', 'CUSTOMER_AMOUNT_1H', 'CUSTOMER_AMOUNT_1W'], \n    [\n        count('TRANSACTION_ID').over(Window.partition_by('CUSTOMER_ID').order_by('TX_DATETIME').range_between(-make_interval(hours=1), Window.currentRow)),\n        count('TRANSACTION_ID').over(Window.partition_by('CUSTOMER_ID').order_by('TX_DATETIME').range_between(-make_interval(weeks=1), Window.currentRow)),\n        sum('TX_AMOUNT').over(Window.partition_by('CUSTOMER_ID').order_by('TX_DATETIME').range_between(-make_interval(hours=1), Window.currentRow)),\n        sum('TX_AMOUNT').over(Window.partition_by('CUSTOMER_ID').order_by('TX_DATETIME').range_between(-make_interval(weeks=1), Window.currentRow))\n    ]\n)",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "step2_customer_fv",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.functions import avg, stddev, count_distinct\n\n# Customer features using Snowpark DataFrame API\ncustomer_window_1h = Window.partition_by('CUSTOMER_ID').order_by('TX_DATETIME').range_between(-make_interval(hours=1), Window.currentRow)\ncustomer_window_24h = Window.partition_by('CUSTOMER_ID').order_by('TX_DATETIME').range_between(-make_interval(hours=24), Window.currentRow)\ncustomer_window_7d = Window.partition_by('CUSTOMER_ID').order_by('TX_DATETIME').range_between(-make_interval(days=7), Window.currentRow)\n\ncustomer_features_df = df.select(\n    col('CUSTOMER_ID'),\n    col('TX_DATETIME'),\n    count('TRANSACTION_ID').over(customer_window_1h).alias('CUST_TX_COUNT_1H'),\n    count('TRANSACTION_ID').over(customer_window_24h).alias('CUST_TX_COUNT_24H'),\n    count('TRANSACTION_ID').over(customer_window_7d).alias('CUST_TX_COUNT_7D'),\n    sum('TX_AMOUNT').over(customer_window_1h).alias('CUST_AMOUNT_1H'),\n    sum('TX_AMOUNT').over(customer_window_24h).alias('CUST_AMOUNT_24H'),\n    avg('TX_AMOUNT').over(customer_window_7d).alias('CUST_AVG_AMOUNT_7D'),\n    stddev('TX_AMOUNT').over(customer_window_7d).alias('CUST_STD_AMOUNT_7D')\n)\n\nprint(\"Customer features DataFrame created with Snowpark API\")\ncustomer_features_df.limit(3).show()\n\ncustomer_fv = FeatureView(\n    name=\"CUSTOMER_FRAUD_FEATURES\",\n    entities=[customer_entity],\n    feature_df=customer_features_df,\n    timestamp_col=\"TX_DATETIME\",\n    refresh_freq=\"1 hour\",\n    desc=\"Customer behavioral features for fraud detection\"\n)\n\ncustomer_fv = customer_fv.attach_feature_desc({\n    \"CUST_TX_COUNT_1H\": \"Number of transactions by customer in last hour\",\n    \"CUST_TX_COUNT_24H\": \"Number of transactions by customer in last 24 hours\",\n    \"CUST_TX_COUNT_7D\": \"Number of transactions by customer in last 7 days\",\n    \"CUST_AMOUNT_1H\": \"Total amount spent by customer in last hour\",\n    \"CUST_AMOUNT_24H\": \"Total amount spent by customer in last 24 hours\",\n    \"CUST_AVG_AMOUNT_7D\": \"Average transaction amount for customer over 7 days\",\n    \"CUST_STD_AMOUNT_7D\": \"Std deviation of transaction amounts for customer over 7 days\"\n})\n\ntry:\n    registered_customer_fv = fs.register_feature_view(\n        feature_view=customer_fv,\n        version=\"v1\",\n        block=True\n    )\n    print(\"✓ Customer feature view registered\")\nexcept Exception as e:\n    registered_customer_fv = fs.get_feature_view(\"CUSTOMER_FRAUD_FEATURES\", \"v1\")\n    print(f\"Customer feature view already exists, retrieved existing\")",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "step2_terminal_fv",
      "metadata": {
        "language": "python"
      },
      "source": "# Terminal features using Snowpark DataFrame API\nterminal_window_1h = Window.partition_by('TERMINAL_ID').order_by('TX_DATETIME').range_between(-make_interval(hours=1), Window.currentRow)\nterminal_window_24h = Window.partition_by('TERMINAL_ID').order_by('TX_DATETIME').range_between(-make_interval(hours=24), Window.currentRow)\nterminal_window_7d = Window.partition_by('TERMINAL_ID').order_by('TX_DATETIME').range_between(-make_interval(days=7), Window.currentRow)\n\nterminal_features_df = df.select(\n    col('TERMINAL_ID'),\n    col('TX_DATETIME'),\n    count('TRANSACTION_ID').over(terminal_window_1h).alias('TERM_TX_COUNT_1H'),\n    count('TRANSACTION_ID').over(terminal_window_24h).alias('TERM_TX_COUNT_24H'),\n    sum('TX_AMOUNT').over(terminal_window_24h).alias('TERM_AMOUNT_24H'),\n    avg('TX_AMOUNT').over(terminal_window_7d).alias('TERM_AVG_AMOUNT_7D'),\n    sum(when(col('TX_FRAUD') == 1, 1).otherwise(0)).over(terminal_window_7d).alias('TERM_FRAUD_COUNT_7D')\n)\n\nprint(\"Terminal features DataFrame created with Snowpark API\")\nterminal_features_df.limit(3).show()\n\nterminal_fv = FeatureView(\n    name=\"TERMINAL_FRAUD_FEATURES\",\n    entities=[terminal_entity],\n    feature_df=terminal_features_df,\n    timestamp_col=\"TX_DATETIME\",\n    refresh_freq=\"1 hour\",\n    desc=\"Terminal behavioral features for fraud detection\"\n)\n\nterminal_fv = terminal_fv.attach_feature_desc({\n    \"TERM_TX_COUNT_1H\": \"Number of transactions at terminal in last hour\",\n    \"TERM_TX_COUNT_24H\": \"Number of transactions at terminal in last 24 hours\",\n    \"TERM_AMOUNT_24H\": \"Total amount processed at terminal in last 24 hours\",\n    \"TERM_AVG_AMOUNT_7D\": \"Average transaction amount at terminal over 7 days\",\n    \"TERM_FRAUD_COUNT_7D\": \"Number of fraudulent transactions at terminal in last 7 days\"\n})\n\ntry:\n    registered_terminal_fv = fs.register_feature_view(\n        feature_view=terminal_fv,\n        version=\"v1\",\n        block=True\n    )\n    print(\"\\u2713 Terminal feature view registered\")\nexcept Exception as e:\n    registered_terminal_fv = fs.get_feature_view(\"TERMINAL_FRAUD_FEATURES\", \"v1\")\n    print(f\"Terminal feature view already exists, retrieved existing\")",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "step2_generate_dataset",
      "metadata": {
        "language": "python"
      },
      "source": "spine_df = session.table(\"TRANSACTIONS_SOURCE\").select(\n    \"TRANSACTION_ID\", \"CUSTOMER_ID\", \"TERMINAL_ID\", \"TX_DATETIME\", \n    \"TX_AMOUNT\", \"TX_FRAUD\", \"HOUR\", \"DAY_OF_WEEK\", \"IS_WEEKEND\", \"IS_NIGHT_12AM_7AM\"\n)\n\ntraining_dataset_v2 = fs.generate_dataset(\n    name=\"FRAUD_TRAINING_V2\",\n    spine_df=spine_df,\n    features=[registered_customer_fv, registered_terminal_fv],\n    version=\"v1\",\n    spine_timestamp_col=\"TX_DATETIME\",\n    spine_label_cols=[\"TX_FRAUD\"],\n    desc=\"Training dataset with customer and terminal features\"\n)\n\nprint(\"✓ Training dataset generated with feature store features\")\ntraining_dataset_v2.read.to_snowpark_dataframe().limit(5).show()",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "step2_train",
      "metadata": {
        "language": "python"
      },
      "source": "df_v2 = training_dataset_v2.read.to_snowpark_dataframe().to_pandas()\n\nfeatures_v2 = [\n    'TX_AMOUNT', 'HOUR', 'DAY_OF_WEEK', 'IS_WEEKEND', 'IS_NIGHT_12AM_7AM',\n    'CUST_TX_COUNT_1H', 'CUST_TX_COUNT_24H', 'CUST_TX_COUNT_7D',\n    'CUST_AMOUNT_1H', 'CUST_AMOUNT_24H', 'CUST_AVG_AMOUNT_7D', 'CUST_STD_AMOUNT_7D',\n    'TERM_TX_COUNT_1H', 'TERM_TX_COUNT_24H', 'TERM_AMOUNT_24H',\n    'TERM_AVG_AMOUNT_7D', 'TERM_FRAUD_COUNT_7D'\n]\n\nX_v2 = df_v2[features_v2].fillna(0).astype(float)\ny_v2 = df_v2['TX_FRAUD'].astype(int)\n\nX_train_v2, X_test_v2, y_train_v2, y_test_v2 = train_test_split(\n    X_v2, y_v2, test_size=0.2, random_state=42, stratify=y_v2\n)\n\nprint(f\"Training set: {X_train_v2.shape[0]} samples\")\nprint(f\"Test set: {X_test_v2.shape[0]} samples\")\nprint(f\"Features ({len(features_v2)}): {features_v2}\")",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step2_model",
      "metadata": {
        "language": "python"
      },
      "source": "model_v2 = XGBClassifier(\n    n_estimators=100,\n    max_depth=5,\n    learning_rate=0.1,\n    random_state=42,\n    eval_metric=\"auc\",\n    tree_method=\"hist\",\n    n_jobs=-1\n)\n\nRUN_NAME = \"feature_store_customer_terminal\"\n\nwith exp.start_run(RUN_NAME):\n    exp.log_params({\n        \"model_type\": \"XGBClassifier\",\n        \"feature_set\": \"customer_terminal\",\n        \"n_features\": len(features_v2),\n        \"n_train_rows\": len(X_train_v2),\n        \"n_test_rows\": len(X_test_v2),\n        \"n_estimators\": 100,\n        \"max_depth\": 5,\n        \"learning_rate\": 0.1,\n        \"eval_metric\": \"auc\",\n        \"feature_views\": \"CUSTOMER_FRAUD_FEATURES, TERMINAL_FRAUD_FEATURES\"\n    })\n\n    model_v2.fit(\n        X_train_v2,\n        y_train_v2,\n        eval_set=[(X_test_v2, y_test_v2)],\n        verbose=False\n    )\n\n    y_pred_v2 = model_v2.predict(X_test_v2)\n    y_proba_v2 = model_v2.predict_proba(X_test_v2)[:, 1]\n\n    metrics_v2 = {\n        \"accuracy\": float(accuracy_score(y_test_v2, y_pred_v2)),\n        \"precision\": float(precision_score(y_test_v2, y_pred_v2, zero_division=0)),\n        \"recall\": float(recall_score(y_test_v2, y_pred_v2, zero_division=0)),\n        \"f1_score\": float(f1_score(y_test_v2, y_pred_v2, zero_division=0)),\n        \"roc_auc\": float(roc_auc_score(y_test_v2, y_proba_v2))\n    }\n    exp.log_metrics(metrics_v2)\n\n    mv_v2 = reg.log_model(\n        model_v2,\n        model_name=\"FRAUD_DETECTION_MODEL\",\n        version_name=\"v2_feature_store\",\n        sample_input_data=X_train_v2.iloc[:min(50, len(X_train_v2))],\n        task=task.Task.TABULAR_BINARY_CLASSIFICATION,\n        metrics=metrics_v2,\n        comment=\"Model with Customer and Terminal feature views from Feature Store\"\n    )\n\nprint(\"\\n=== Feature Store Model Results ===\")\nfor metric, value in metrics_v2.items():\n    print(f\"{metric}: {value:.4f}\")",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "step3_header",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n# Step 3: Model Comparison & Set Best as Default\n\nCompare both models and set the best performing version as the default in the model registry."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step3_compare",
      "metadata": {
        "language": "python"
      },
      "source": "comparison_data = {\n    'Version': ['v1_demo_basic', 'v2_feature_store'],\n    'Features': ['Basic (5)', 'Customer+Terminal (17)'],\n    'Accuracy': [metrics_baseline['accuracy'], metrics_v2['accuracy']],\n    'Precision': [metrics_baseline['precision'], metrics_v2['precision']],\n    'Recall': [metrics_baseline['recall'], metrics_v2['recall']],\n    'F1 Score': [metrics_baseline['f1_score'], metrics_v2['f1_score']],\n    'ROC AUC': [metrics_baseline['roc_auc'], metrics_v2['roc_auc']]\n}\n\ncomparison_df = pd.DataFrame(comparison_data)\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL COMPARISON - Feature Engineering Impact on Fraud Detection\")\nprint(\"=\"*80)\nprint(comparison_df.to_string(index=False))\nprint(\"=\"*80)",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step3_improvement",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\\nIMPROVEMENT ANALYSIS:\")\nprint(\"-\" * 50)\n\nfor metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']:\n    baseline = comparison_df[comparison_df['Version'] == 'v1_demo_basic'][metric].values[0]\n    best = comparison_df[metric].max()\n    best_version = comparison_df[comparison_df[metric] == best]['Version'].values[0]\n    if baseline > 0:\n        improvement = ((best - baseline) / baseline) * 100\n        print(f\"{metric:12} | Baseline: {baseline:.4f} -> Best: {best:.4f} ({best_version}) | +{improvement:.2f}%\")\n    else:\n        print(f\"{metric:12} | Baseline: {baseline:.4f} -> Best: {best:.4f} ({best_version})\")",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step3_set_default",
      "metadata": {
        "language": "python"
      },
      "source": "all_metrics = {\n    'v1_demo_basic': metrics_baseline,\n    'v2_feature_store': metrics_v2\n}\n\nbest_version = max(all_metrics.keys(), key=lambda v: all_metrics[v]['f1_score'])\nbest_f1 = all_metrics[best_version]['f1_score']\n\nprint(f\"\\nBest performing model: {best_version}\")\nprint(f\"   F1 Score: {best_f1:.4f}\")\n\nmodel = reg.get_model(\"FRAUD_DETECTION_MODEL\")\nmodel.default = best_version\n\nprint(f\"\\nModel registry default version set to: {best_version}\")\nprint(f\"\\nModel versions in registry:\")\nmodel.show_versions()",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step3_summary",
      "metadata": {
        "language": "python"
      },
      "source": "print(\"\"\"\n================================================================================\n                           DEMO SUMMARY\n================================================================================\n\nExperiment Tracking: Both model runs logged to 'FRAUD_DETECTION_FEATURE_ENGINEERING'\nFeature Store: Created CUSTOMER and TERMINAL entities with feature views\nModel Registry: 2 versions logged to 'FRAUD_DETECTION_MODEL'\nDefault Version: Set to best performing model based on F1 score\n\nKEY TAKEAWAYS:\n1. Feature engineering improves fraud detection performance\n2. Customer behavioral features capture spending patterns and velocity\n3. Terminal features identify high-risk locations and fraud patterns\n\nNEXT STEPS:\n- View experiment results in Snowsight: AI & ML > Experiments\n- View model registry: AI & ML > Model Registry\n- Enable ML Observability to monitor model drift in production\n\n================================================================================\n\"\"\")",
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}